%--------------------
% Packages
% -------------------
\documentclass[11pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{gentium}
%\usepackage{mathptmx} % Use Times Font

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}



\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand{\irow}[1]{% inline row vector
  \begin{smallmatrix}(#1)\end{smallmatrix}%
}

\newcommand{\TODO}[2]{ {\small \bf (!)} {#1} + {#2}}

\usepackage[pdftex]{graphicx} % Required for including pictures
\usepackage[english]{babel} % Swedish translations
\usepackage[pdftex,linkcolor=blue,pdfborder={0 0 0}]{hyperref} % Format links for pdf
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    }

\usepackage{calc} % To reset the counter in the document after title page
\usepackage{enumitem} % Includes lists

\frenchspacing % No double spacing between sentences
\linespread{1.2} % Set linespace
\usepackage[a4paper, lmargin=0.1666\paperwidth, rmargin=0.1666\paperwidth, tmargin=0.1111\paperheight, bmargin=0.1111\paperheight]{geometry} %margins
%\usepackage{parskip}

\usepackage{amsfonts}

\usepackage[all]{nowidow} % Tries to remove widows
\usepackage[protrusion=true,expansion=true]{microtype} % Improves typography, load after fontpackage is selected

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{fact}{Fact}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

%-----------------------
% Set pdf information and add title, fill in the fields
%-----------------------
\hypersetup{ 	
pdfsubject = {},
pdftitle = {},
pdfauthor = {}
}

%-----------------------
% Begin document
%-----------------------
\begin{document} %All text i dokumentet hamnar mellan dessa taggar, allt ovanför är formatering av dokumentet


\section*{Preamble 1: Gaussian Processes for Vector Valued Functions}
In this section, we fix some notation for the rest of the document, by describing Gaussian Processes (GPs) for vector valued functions as in Alvarez et al. (\textit{Kernels for vector valued functions: a Review}, 2011). \\

Recall that a GP can be defined as a stochastic process such that any finite number of random variables taken from a realization of the GP follows a joint Gaussian distribution. In the 1D output case, this characterization implies that for any finite set $\textbf{X} = \{\textbf{x}_n\}_{n=1}^N$, if we let $f(\textbf{X}) = [f(\textbf{x}_1), \ldots, f(\textbf{x}_N)]^T$, then 
$$
f(\textbf{X}) \sim \mathcal{N}(m(\textbf{X}), k(\textbf{X}, \textbf{X}))
$$
where $m(\textbf{X}) = [m(\textbf{x}_1), \ldots, m(\textbf{x}_N)]^T$ and $k(\textbf{X}, \textbf{X})$ is a $N \times N$ kernel matrix.

In the multiple output case, the random variables are associated to different processes $\{f_d(\textbf{X})\}_{d=1}^D$, evaluated at input values $\textbf{X}$. Define $f(\textbf{X)} = [f_1(\textbf{X}), \ldots, f_D(\textbf{X})]^T$, a N$\times$D dimensional vector, where each entry $f_d(\textbf{X})$ is N-dimensional as in the 1D framework above. Note that here we're making the simplifying assumption that each component has the same cardinality, and the input to all the components are the same, as in our problem of interest. Then, the GP characterization generalizes to 
$$
f(\textbf{X}) \sim \mathcal{N}(\textbf{m}(\textbf{X}), \textbf{K}(\textbf{X}, \textbf{X}))
$$
where (i) $\textbf{m}(\textbf{X)}$ is a vector of dimension $ND$ that concatenates the mean vectors associated to the outputs $\allowdisplaybreaks \{m_d(\textbf{X})\}_{d=1}^D$, i.e. $\textbf{m}(\textbf{X)} = [m_1(\textbf{x}_1), \ldots, m_1(\textbf{x}_N), m_2(\textbf{x}_1), \ldots, m_D(\textbf{x}_1), \ldots, m_D(\textbf{x}_N)]^T$, and (ii) $\textbf{K}(\textbf{X}, \textbf{X})$ is a block-partitioned $ND \times ND$ matrix,
$$
\textbf{K}(\textbf{X}, \textbf{X}) = 
\begin{bmatrix}
\textbf{K}_{1,1}(\textbf{X}, \textbf{X}) & \ldots & \textbf{K}_{1,D}(\textbf{X}, \textbf{X}) \\  \textbf{K}_{2,1}(\textbf{X}, \textbf{X}) & \ldots & \textbf{K}_{2,D}(\textbf{X}, \textbf{X}) \\
\vdots & \ldots & \vdots \\
\textbf{K}_{D,1}(\textbf{X}, \textbf{X}) & \ldots & \textbf{K}_{D,D}(\textbf{X}, \textbf{X})
\end{bmatrix}
 $$
 where each block $\textbf{K}(\textbf{X}, \textbf{X})_{d,d^\prime}$ is an $N \times N$ kernel  matrix such that the entries $\textbf{K}(\textbf{x}, \textbf{x}^\prime)_{d,d^\prime}$ correspond to the covariances between the outputs $f_d(\textbf{x})$ and $f_{d^\prime}(\textbf{x}^\prime)$, and express the degree of correlation or similarity between them. \\
 

\section*{Preamble 2: Helmholtz decomposition}

Here, review main concepts about Helmholtz decomposition in a somewhat coincise way, following  \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=6365629}{this} review paper. 


\section*{Gaussian Processes for 2D oceans vector fields}

In our problem of interest, we are given a 2D vector field F : $ \mathbb{R}^2 \rightarrow \mathbb{R}^2$, mapping spatial locations, $(x^1,x^2)$, into horizontal and vertical velocities, $(F^1,F^2)$, representing buoys floating across the oceans. In terms of data, we have M observations, at locations $\textbf{X} = \{\textbf{x}_m\}_{m=1}^M = \{(x_1^1, x_1^2), \ldots, (x_M^1, x_M^2)\}$, and the corresponding vector field velocity observations, $\textbf{F}(\textbf{X}) = [\textbf{F}_1, \textbf{F}_2, \ldots, \textbf{F}_M] := [(F^1(\textbf{x}_1), F^2(\textbf{x}_1)), \ldots,  (F^1(\textbf{x}_M), F^2(\textbf{x}_M))]$, where $F^1(\textbf{x}_m)$ represents the horizontal velocity at location $\textbf{x}_m$, and $F^2(\textbf{x}_m)$ the vertical one. 


Our ultimate goal is to reconstruct ocean currents given these data, that is, predict the velocity field at spatial locations where we don't observe any buoy, with some measure of uncertainty about the quality of our prediction. 

Independently for each m, we model the likelihood for our quantities of interest as
$$
(F^1(\textbf{x}_m), F^2(\textbf{x}_m)) \stackrel{\text{ind}}{\sim} \mathcal{N}\left(\text{\textbf{F}}_m, \begin{bmatrix}
\sigma^2_{\text{obs}} & 0 \\ 0 & \sigma^2_{\text{obs}}
\end{bmatrix} \right)
$$
where $\sigma^2_{\text{obs}}$ represent the variance of our observations.

In order to characterize the posterior distribution, we need to specify a prior for $\textbf{F}_m$. Here is where the Helmholtz decomposition comes into play. According to this result, any sufficiently smooth, rapidly decaying vector field in two dimensions can be resolved into the sum of an irrotational (curl-free) scalar vector field and a solenoidal (divergence-free) vector field. 

As an irrotational vector field has a scalar potential and a solenoidal vector field has a vector potential, we can formally write down the decomposition as 
$$
 \text{\textbf{F}} = -\nabla \bm{\phi} +\nabla \times \bm{\psi}
$$
 where $\bm{\phi} : \mathbb{R}^2 \rightarrow \mathbb{R}$ is the scalar potential, and $\bm{\psi}$ the vector potential. In abuse of notation, we consider $\bm{\psi} : \mathbb{R}^2 \rightarrow \mathbb{R}$, and define $(\nabla \times \bm{\psi})(\textbf{x}) := [\partial \bm{\psi}(\textbf{x}) / \partial x^2, -\partial \bm{\psi}(\textbf{x}) / \partial x^1]^T$.
 
 
 In our work, we model these two quantities \textbf{independently}, so that we can naturally specify two distinct GP priors on them, that is
\begin{align*}
    \bm{\phi} \sim \text{GP}(0, K_{\phi}) \\
    \bm{\psi} \sim \text{GP}(0,K_{\psi}) \\
\end{align*}
so that 
\begin{align*}
    \bm{\phi}(\textbf{X}) \sim \mathcal{N}(0, K_{\phi}(\textbf{X}, \textbf{X})) \\
    \bm{\psi}(\textbf{X}) \sim \mathcal{N}(0, K_{\psi}(\textbf{X}, \textbf{X}))
\end{align*}


Note that - to ease notation - from now on we will denote this kernel matrices with $K_{\phi}(\textbf{X}) := K_{\phi}(\textbf{X}, \textbf{X})$. \newline

We can show (see Theorem below  - \textit{when it'll be ready...}) that since (i) the Helmholtz decomposition is based on partial (mixed) derivatives, and (ii) differentiation is a linear operator, this kernel choice implies that the prior on the original vector field F is a GP as well. To formalize this statement, we need the following definition and lemma. 

\begin{definition}
L is a linear operator if $\forall c \in \mathbb{R}$, and $F, F^\prime \in L^2$, $L(cF + F^\prime) = c L(F) + L(F^\prime)$.
\end{definition}

\begin{lemma}
Let L be a linear operator. If $F \sim \text{GP} (\mu, K)$, then $L(F) \sim \text{GP}(\mu^\prime, K^\prime)$ for some $(\mu^\prime, K^\prime)$.
\end{lemma}

\begin{proof}
Proof (in $L^2$). 
\end{proof}

\begin{corollary}
Let $F \sim \text{GP}(\mu, K)$. Then $\nabla F \sim GP (\nabla \mu, \nabla K)$, where $\nabla \mu = ...$, $\nabla K = ...$.
\end{corollary}

\begin{proof}
Use above lemma plus Rasmussen plus theorem at the end of this document (that will then be useless?). 
\end{proof}


As a consequence of this, if we consider two observations' indices $m$ and $m^\prime$, and we denote with $\text{Cov}$ the \textit{cross}-covariance, we have that 
\begin{align*}
    \text{Cov}(\textbf{F}_m, \textbf{F}_{m^\prime})  
    &= \text{Cov}(-\nabla \phi(\textbf{x}_m) + \nabla \times \psi(\textbf{x}_m) , -\nabla \phi(\textbf{x}_{m^\prime})+\nabla \times \psi(\textbf{x}_{m^\prime}) \\
    &= \text{Cov}(\nabla \phi(\textbf{x}_m), \nabla \phi(\textbf{x}_{m^\prime})) + \text{Cov}(\nabla \times \psi(\textbf{x}_m), \nabla \times \psi(\textbf{x}_{m^\prime}))
\end{align*}
where the last equality follows from the independence of $\psi$ and $\phi$. 

Furthermore, the following result on the mean of a Gaussian process can be shown. 

\begin{lemma}
Let $c \in \mathbb{R}$, $F \sim \text{GP}(\mu, K)$, and $F^\prime \sim \text{GP}(\mu^\prime, K^\prime)$. Under "some" (which?) assumptions, $cF + F^\prime$ is a GP itself, with mean $c\mu + \mu^\prime$.
\end{lemma}


\begin{proof}
even though trivial, for this we might need some assumptions on when to switch expectation and derivative? maybe this is an issue of the lemma above actually... The condition should be something like Lipshitz continuity with high probability. 
Can't we just use the above lemma on linear operators?
\end{proof}


Putting everything together, our prior on the original vector field F is a GP with the following structure
\begin{equation}
    \text{F}(\textbf{X}) \sim \text{GP}(0, \nabla K_{\phi}(\textbf{X}) + \nabla \times K_{\psi}(\textbf{X}))
\end{equation}



where we use $\nabla K_{\phi}(\textbf{X})$ as a shorthand for the 2N $\times$ 2N matrix with entries $\nabla K_{\phi}(\textbf{X})_{n,n^\prime} := \text{Cov}\left(\frac{\partial}{\partial x_n^1} \phi(\textbf{x}_n), \frac{\partial}{\partial x_n^1} \phi(\textbf{x}_n^\prime)\right)$, $\nabla K_{\phi}(\textbf{X})_{n,N + n^\prime} := \text{Cov}\left(\frac{\partial}{\partial x_n^1} \phi(\textbf{x}_n), \frac{\partial}{\partial x_n^2} \phi(\textbf{x}_n^\prime)\right)$, $\nabla K_{\phi}(\textbf{X})_{N + n,n^\prime} := \text{Cov}\left(\frac{\partial}{\partial x_n^2} \phi(\textbf{x}_n), \frac{\partial}{\partial x_n^1} \phi(\textbf{x}_n^\prime)\right)$, and $\nabla K_{\phi}(\textbf{X})_{N + n, N + n^\prime} := \text{Cov}\left(\frac{\partial}{\partial x_n^2} \phi(\textbf{x}_n), \frac{\partial}{\partial x_n^2} \phi(\textbf{x}_n^\prime)\right)$, for $n, n^\prime \in [N]$. \newline

%Note that is easy to be dealt with, once we specify the prior kernels on $\phi$ and $\psi$.

\textbf{Note:} little abuse of notation in the derivative of second component in the covariance terms above. It should be more something like $\frac{\partial}{\partial x_n^{\prime^2}} \phi(\textbf{x}_n^\prime)$.

\section*{Inferring $\phi$ and $\psi$ components of the posterior}

Let F be our vector field of interest. Through the Helmholtz decomposition, we write it as 
$$
\text{F} = \nabla \phi + \nabla \times \psi
$$
where $\phi$ is the irrotational, curl-free, component, and $\psi$ a divergence-free vector potential. In our analysis, we evaluate our predictions on a grid of size $G \times G := N$, and we want to know $\phi$ and $\psi$ at each point of the grid, e.g. for visualization purposes. That is, we are interested in $\bm{\phi} \in \mathbb{R}^N$, where $\bm{\phi} = [\phi_1, \ldots, \phi_N]$, $\phi_n := \phi(\textbf{x}_n)$ for each $n \in [N]$, and similarly for $\bm{\psi} \in \mathbb{R}^n$.

Our goal is to plot $\nabla \phi \in \mathbb{R}^2$ and $\nabla \times \psi \in \mathbb{R}^2$, for each grid point of interest. To achieve this, we need to derive the posterior distribution of $\bm{\phi} \mid \textbf{F}^\prime$, where $\textbf{F}^\prime := [F^1(\textbf{x}^\prime_1), \ldots, F^1(\textbf{x}^\prime_M), F^2(\textbf{x}^\prime_1), \ldots, F^2(\textbf{x}^\prime_M) ] \in \mathbb{R}^{2M}$ is a vector containing the observations of our vector field $F$ at $M$ given spatial points. In other words, we are simply interested in retrieving $\mathbb{E}[\bm{\phi}\mid\textbf{F}^\prime]$ and $\text{Cov}[\bm{\phi}\mid\textbf{F}^\prime]$ from $\icol{\bm{\phi} \\ \textbf{F}^\prime}$.  For simplicity, in this exposition we focus only on the $\phi$ component, but at the end of the section we will show how these results can be adapted to the $\psi$ component. \newline

Recall the following basic probability facts.
\begin{fact}
If 
$$
\begin{bmatrix}
X \\ Y
\end{bmatrix}
\sim \mathcal{N}\left(0,
\begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22} \\
\end{bmatrix}
\right)
$$
then $X \mid Y = y \sim \mathcal{N}(\Bar{\mu}, \Bar{\Sigma})$, with $\Bar{\mu} = \Sigma_{12} \Sigma_{22}^{-1}y$ and $\Bar{\Sigma} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1}\Sigma_{21}$.

\end{fact}

\begin{fact}
If $a \sim \mathcal{N}(\mu_a, \Sigma_a)$ and $a^{\prime} \sim \mathcal{N}(\mu_{a^{\prime}}, \Sigma_{a^{\prime}})$ are two independent random variables, and $b \mid a, a^{\prime} \sim \mathcal{N}(\mu_a + \mu_{a^{\prime}}, \Sigma_b)$, then $b \sim \mathcal{N}(\mu_a + \mu_{a^{\prime}}, \Sigma_a + \Sigma_{a^{\prime}} + \Sigma_b)$.
\end{fact}

In order to use these in our problem of interest, we need to express the distribution of $\icol{\bm{\phi} \\ \textbf{F}}$. 


We begin considering the random vector $\bm{\phi}$. By construction, $\bm{\phi}$ is a vector of size $N$, with mean $\textbf{0} \in \mathbb{R}^{N}$ and covariance matrix of size $N \times N$ determined by $K_{\phi}$. 

Next, we consider $\bm{F}^\prime$. We have 
$$
\textbf{F}^\prime_m
\mid \phi, \psi \sim \mathcal{N}\left(\nabla \phi (\textbf{x}_m) + \nabla \times \psi (\textbf{x}_m), \sigma^2_{\text{obs}} \mathbb{I}_2\right)
$$
where $\mathbb{I}_2$ represents the 2 $\times$ 2 identity matrix. 
Then, we can use fact 2 above with 
\begin{align*}
   &\nabla \phi (\textbf{X}) \sim \mathcal{N}(0, \nabla K_{\phi}(\textbf{X})) \\
   &\nabla \times \psi (\textbf{X}) \sim \mathcal{N}(0, \nabla \times K_{\psi}(\textbf{X}))
\end{align*}

to get
$$
\textbf{F}^\prime
\sim \mathcal{N}\left(0, \sigma_{\text{obs}}^2 \mathbb{I}_{2M} + \nabla K_{\phi}(\textbf{X}) + \nabla \times K_{\psi}(\textbf{X}) \right)
$$
where each of the matrices has dimension $2M \times 2M$, and the entries are obtained as in Equation (1) from previous section. \newline

%\textbf{ISSUE 1.1:} I am abusing notation for \textbf{X}. Here it's of dimension M, data observed, whereas before it was intended of dimension $N^2$, predicted grid. Fix this. \newline

%\textbf{ISSUE 1.2:} By using the "theorem" below, if we unpack the actual $\nabla K_{\phi}((x_m, y_m), (x_{m^\prime},y_{m^\prime}))$ we see that it looks more like a second derivative rather than a first derivative (each entry is a second derivative w.r.t. the corresponding components). Wouldn't it be better to call it H, for Hessian? Rather than using the $\nabla$ symbol for the gradient.  \newline

%\textbf{ISSUE 1.3:} I only have m points/observations. The covariance matrices have dimension $2M \times 2M$ because the gradient "doubles" the dimension, since it is partial derivative w.r.t. both entries. Question: if I would like to be super explicit in writing down these matrices, how would they be? Alternatively, considering a more computational approach, how are they stored in a computer memory? \newline

This concludes two blocks of the covariance matrix of $\icol{\bm{\phi} \\ \textbf{F}^\prime}$. Now we need to take into account the cross terms, i.e. the cross covariance matrix $\text{Cov}(\bm{\phi}, \textbf{F}^\prime)$. Since we have $N$ points on the grid, and $M$ observations, this matrix will have size $N \times 2M$, properly fitting in the "hole" in the general covariance matrix of interest. 

Overall, we have 

\begin{align*}
    \text{Cov}\left(\bm{\phi}, \textbf{F}^\prime \right) = 
\text{Cov}\left( \begin{bmatrix}
\phi(\textbf{x}_1) \\ \phi(\textbf{x}_2) \\ \vdots \\ \phi(\textbf{x}_N)
\end{bmatrix}, \begin{bmatrix} 
[F^1(\textbf{x}^\prime_1) \\ \vdots \\ F^1(\textbf{x}^\prime_M) \\ F^2(\textbf{x}^\prime_1) \\ \vdots \\ F^2(\textbf{x}^\prime_M) ]
\end{bmatrix}
\right) &= \\ 
\begin{bmatrix}
\text{Cov}(\phi(\textbf{x}_1),F^1(\textbf{x}_1^\prime)) & \hdots & \text{Cov}(\phi(\textbf{x}_1),F^2(\textbf{x}_M^\prime)) \\
\vdots & \vdots & \vdots \\
\text{Cov}(\phi(\textbf{x}_n),F^1(\textbf{x}_1^\prime)) & \hdots & \text{Cov}(\phi(\textbf{x}_n),F^2(\textbf{x}_M^\prime)) 
\end{bmatrix}
\end{align*}
\newline

%\textbf{ISSUE 2.1:} again, abuse of notation here. We need to be more explicit about the two different X's. \newline

%\textbf{ISSUE 2.2:} is it correct the way in which we represent the vector $\icol{u \\ v }$? Or should it be first all the u's and then all the v's? \newline


Now we just need to characterize each of these entries, as follows:

\begin{align*}
    \text{Cov}\left(\phi(\textbf{x}_n), F^1(\textbf{x}_m^\prime)\right) &=    \text{Cov}\left(\phi(\textbf{x}_n), -\frac{\partial}{\partial x_m^{1}} \phi(\textbf{x}_m^\prime) +\frac{\partial}{\partial x_m^2} \psi(\textbf{x}_m^\prime)  \right) \\
    &\text{\# by independence of $\phi$ and $\psi$} \\
   &= \text{Cov}\left(\phi(\textbf{x}_n), -\frac{\partial}{\partial x_m^{1}} \phi(\textbf{x}_m^\prime) \right) \\
   &= 
   -\frac{\partial}{\partial x_m^{1}} \text{Cov}(\phi(\textbf{x}_n), \phi(\textbf{x}_m^\prime)) \\
    &\text{\# by lemma above (cite number)} \\
    &= 
    -\frac{\partial}{\partial x_m^{1}} K_{\phi}(\textbf{x}_n, \textbf{x}_m^\prime)
\end{align*}

and similarly 

\begin{align*}
    \text{Cov}\left(\phi(\textbf{x}_n), F^2(\textbf{x}_m^\prime)\right) &=    \text{Cov}\left(\phi(\textbf{x}_n), -\frac{\partial}{\partial x_m^{2}} \phi(\textbf{x}_m^\prime) - \frac{\partial}{\partial x_m^1} \psi(\textbf{x}_m^\prime)  \right) \\
   &=
    -\frac{\partial}{\partial x_m^{2}} K_{\phi}(\textbf{x}_n, \textbf{x}_m^\prime)
\end{align*}

To ease notation, we define 
$$
\nabla_{D_M}K_{\phi}(\textbf{x}_n, \textbf{x}_m^\prime) := \text{Cov}\left(\phi(\textbf{x}_n), F(\textbf{x}_m^\prime)\right) = 
\begin{bmatrix}
-\frac{\partial}{\partial x_m^{1}} K_{\phi}(\textbf{x}_n, \textbf{x}_m^\prime) \\
-\frac{\partial}{\partial x_m^{2}} K_{\phi}(\textbf{x}_n, \textbf{x}_m^\prime)
\end{bmatrix}
$$

where we used this notation because we refer to the test points (grid) with $D_N := (\textbf{x}_n)_{n=1}^N$, and to the observation with $D_M := (\textbf{x}_m)_{m=1}^M$. \newline

So overall we have
$$
\begin{bmatrix}
\bm{\phi} \\ \textbf{F}^\prime 
\end{bmatrix}
\sim \mathcal{N}\left( 
\begin{bmatrix}
0 \\ 0 
\end{bmatrix}, 
\begin{bmatrix}
K_{\phi} & \nabla_{D_M} K_{\phi} \\
\nabla_{D_M} K_{\phi}^T & \sigma^2 \mathbb{I}_{2M} + \nabla K_{\phi} + \nabla \times K_{\psi}
\end{bmatrix}
\right)
$$

This vector has size $N + 2M$, with the caveat that $\bm{\phi}$ is evaluated at test points $D_N$ whereas $\textbf{F}^\prime$ at the observations $D_M$. \newline

We can then, finally, get the quantity of interest, $\bm{\phi} \mid \textbf{F}^\prime$, applying fact 1 above. We obtain 
$$
\mathbb{E}(\bm{\phi} \mid \textbf{F}^\prime) = \nabla_{D_M} K_{\phi} (\sigma^2 \mathbb{I}_{2M} + \nabla K_{\phi} + \nabla \times K_{\psi})^{-1} \textbf{F}^\prime
$$
which is of dimension $N$, exactly as desired, and 
$$
\text{Var}(\bm{\phi} \mid \textbf{F}^\prime) = K_{\phi} - \nabla_{D_M} K_{\phi} (\sigma^2 \mathbb{I}_{2M} + \nabla K_{\phi} + \nabla \times K_{\psi})^{-1} \nabla_{D_M} K_{\phi}^T
$$
with dimension $N \times N$, again as desired. \newline

The same exact analysis can then be generalized to $\psi$, leading to 
$$
    \bm{\psi} \mid \textbf{F}^\prime \sim \mathcal{N}(\Bar{\mu}_{\psi}, \Bar{\Sigma}_{\psi})
$$
with 
\begin{align*}
    &\Bar{\mu}_{\psi} = \nabla_{D_M} K_{\psi} (\sigma^2 \mathbb{I}_{2m} + \nabla K_{\phi} + \nabla \times K_{\psi})^{-1} 
    \textbf{F}^\prime \\
    &\Bar{\Sigma}_{\psi} = K_{\psi} - \nabla_{D_M} K_{\psi} (\sigma^2 \mathbb{I}_{2m} + \nabla K_{\phi} + \nabla \times K_{\psi})^{-1} \nabla_{D_M} K_{\psi}^T
\end{align*}
for $\nabla_{D_M} K_{\psi}$ defined similarly to what we did for $\phi$ above. This concludes our analysis for this section. \newline

\newpage

\section*{Derivative observations on Gaussian Processes - notation to be fixed, proof to be fixed...}

\begin{theorem}
Consider a Gaussian process with mean $\mu$ and covariance kernel $K$ and let $\phi$ be a differentiable function, mapping $\mathbb{R}^2$ into $\mathbb{R}$, drawn from this GP, i.e. $\phi \sim \textit{GP}(\mu, K)$. Given two pairs of inputs, $(x_i,y_i)$ for $i=1,2$, such that $\phi(x_1,y_1) \sim \textit{GP}(\mu(x_1,y_1), K((x_1,y_1), (x_2,y_2))$, then $\nabla \phi$ is itself a Gaussian process with mean $\nabla \mu$ and covariance
$$
\begin{bmatrix}
\frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial x_1 \partial x_2} & \frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial x_1 \partial y_2}\\
\frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial y_1 \partial x_2} & \frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial y_1 \partial y_2}
\end{bmatrix}
$$
\end{theorem}

\begin{proof}
First of all, observe that since differentiation is a linear operator, the derivative of a Gaussian process is another Gaussian process. In particular, according to Rasmussen and Williams, a covariance function $K(\cdot, \cdot)$ on function values implies the following covariance between function values and partial derivatives, and between partial derivatives:
\begin{align*}
    \textit{Cov}\left(f_i, \frac{\partial f_j}{\partial x_{dj}}\right) &= \frac{\partial K(x_i, x_j)}{\partial x_{dj}} \\
    \textit{Cov}\left(\frac{\partial f_i}{\partial x_{di}}, \frac{\partial f_j}{\partial x_{dj}}\right) &= \frac{\partial^2 K(x_i, x_j)}{\partial x_{di}\partial x_{dj}} 
\end{align*}
If we apply this result in our setting, where $\nabla \phi = \left[ \frac{\partial \phi}{\partial x} \hspace{3pt} \frac{\partial \phi}{\partial y}  \right]^T$, we obtain:

\begin{align*}
    \textit{Cov}(\nabla \phi (x_1, y_1), \nabla \phi (x_2, y_2)) &= \textit{Cov} \left( 
    \begin{bmatrix}
    \frac{\partial \phi (x_1, y_1)}{\partial x_1} \\ \frac{\partial \phi (x_1, y_1)}{\partial y_1}
    \end{bmatrix}, 
    \begin{bmatrix}
    \frac{\partial \phi (x_2, y_2)}{\partial x_2} \\ \frac{\partial \phi (x_2, y_2)}{\partial y_2}
    \end{bmatrix}
    \right) = \\
    &= 
    \begin{bmatrix}
    \textit{Cov} \left( \frac{\partial \phi (x_1, y_1)}{\partial x_1}, \frac{\partial \phi (x_2, y_2)}{\partial x_2} \right) & \textit{Cov} \left( \frac{\partial \phi (x_1, y_1)}{\partial x_1}, \frac{\partial \phi (x_2, y_2)}{\partial y_2} \right) \\
    \textit{Cov} \left( \frac{\partial \phi (x_1, y_1)}{\partial y_1}, \frac{\partial \phi (x_2, y_2)}{\partial x_2} \right) & \textit{Cov} \left( \frac{\partial \phi (x_1, y_1)}{\partial y_1}, \frac{\partial \phi (x_2, y_2)}{\partial y_2} \right)
    \end{bmatrix} = \\
    &= 
    \begin{bmatrix}
    \frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial x_1 \partial x_2} & \frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial x_1 \partial y_2}\\
    \frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial y_1 \partial x_2} & \frac{\partial^2 K((x_1,y_1), (x_2,y_2))}{\partial y_1 \partial y_2}
\end{bmatrix}
\end{align*}

as desired. 
\end{proof}


\end{document}
